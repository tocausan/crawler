from  scrapy.item import Item
from scrapy import log
from scrapy.contrib.spiders import XMLFeedSpider
from search ngine.items import Spider_Crawls

class SpiderCrawls():
	id = http://tomascaufriezsanchez.com/'
	name = 'tomascaufriezsanchez.com/'
	allowed_domains = ['tomascaufriezsanchez.com/']
	start_urls = [http://tomascaufriezsanchez.com/']
	description = field()

	void spidercrawls(void)
    {
    
    CkSpider spider;

    //  The spider object crawls a single web site at a time.  As you'll see
    //  in later examples, you can collect outbound links and use them to
    //  crawl the web.  For now, we'll simply spider 10 pages of chilkatsoft.com
    spider.Initialize("www.chilkatsoft.com");

    //  Add the 1st URL:
    spider.AddUnspidered("http:/http://tomascaufriezsanchez.com/");

    //  Begin crawling the site by calling CrawlNext repeatedly.
    long i;
    for (i = 0; i <= 9; i++) {
        bool success;
        success = spider.CrawlNext();
        if (success == true) {
            //  Show the URL of the page just spidered.
            printf("%s\n",spider.lastUrl());

            //  The HTML META keywords, title, and description are available in these properties:
            printf("%s\n",spider.lastHtmlTitle());
            printf("%s\n",spider.lastHtmlDescription());
            printf("%s\n",spider.lastHtmlKeywords());

            //  The HTML is available in the LastHtml property
        }
        else {
            //  Did we get an error or are there no more URLs to crawl?
            if (spider.get_NumUnspidered() == 0) {
                printf("No more URLs to spider\n");
            }
            else {
                printf("%s\n",spider.lastErrorText());
            }

        }

        //  Sleep 1 second before spidering the next URL.
        spider.SleepMs(1000);
    }


    }

